# chat-agent

Projeto de API de chat usando **FastAPI** + **Strands Agents** + **Ollama**, desenvolvido como soluÃ§Ã£o para o desafio tÃ©cnico.

O agente de IA Ã© capaz de:
- Responder perguntas de **conhecimento geral** (sem usar ferramentas externas).
- Identificar quando a pergunta exige **cÃ¡lculo matemÃ¡tico** e, nesses casos, utilizar a tool `calculator` do Strands para obter o resultado com precisÃ£o.

---

## ğŸ“ Estrutura do projeto

```bash
chat-agent/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ agent.py          # ConfiguraÃ§Ã£o do agente Strands + Ollama + tools
â”‚   â”œâ”€â”€ config.py         # Leitura das variÃ¡veis de ambiente (.env)
â”‚   â”œâ”€â”€ main.py           # AplicaÃ§Ã£o FastAPI (endpoints)
â”‚   â””â”€â”€ schemas.py        # Modelos Pydantic de request/response
â”œâ”€â”€ .env.example          # Exemplo de variÃ¡veis de ambiente
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt

âœ… PrÃ©-requisitos

Python 3.10+ instalado

Ollama instalado localmente
ğŸ‘‰ DocumentaÃ§Ã£o: https://ollama.com

âš™ï¸ ConfiguraÃ§Ã£o do ambiente
1. Clonar o repositÃ³rio

git clone https://github.com/EmersonCarlosAlvesSouza/chat-agent.git
cd chat-agent
2. Criar e ativar o ambiente virtual
python -m venv .venv

# Windows (PowerShell)
.venv\Scripts\Activate.ps1

# Windows (CMD)
.venv\Scripts\activate.bat

# Linux / macOS
source .venv/bin/activate

3. Instalar dependÃªncias
pip install -r requirements.txt

4. Instalar o modelo no Ollama

Exemplo usando o modelo llama3.1:

ollama pull llama3.1

5. Configurar variÃ¡veis de ambiente

Copie o arquivo de exemplo:

cp .env.example .env


Edite o arquivo .env se quiser alterar alguma configuraÃ§Ã£o:

# Ollama configuration
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL_ID=llama3.1:latest

# Agent configuration
AGENT_SYSTEM_PROMPT=VocÃª Ã© um assistente Ãºtil que responde em portuguÃªs do Brasil. Para perguntas gerais (histÃ³ria, biografias, ciÃªncia, curiosidades, etc.), responda normalmente SEM usar ferramentas. Use a ferramenta de cÃ¡lculo `calculator` SOMENTE quando a pergunta envolver cÃ¡lculos matemÃ¡ticos numÃ©ricos explÃ­citos (operaÃ§Ãµes, raÃ­zes, potÃªncias, porcentagens, etc.). Ao chamar a ferramenta, passe apenas a expressÃ£o matemÃ¡tica bruta, por exemplo: "1234 * 5678" ou "sqrt(144)", sem texto extra. Nunca chame a ferramenta `calculator` para perguntas sobre pessoas, histÃ³ria ou conceitos nÃ£o numÃ©ricos.

ğŸš€ Executando a API

Com o ambiente virtual ativado e o Ollama rodando:

uvicorn app.main:app --reload


A API ficarÃ¡ disponÃ­vel em:

http://localhost:8000

DocumentaÃ§Ã£o automÃ¡tica do FastAPI (opcional):

Swagger UI: http://localhost:8000/docs

ReDoc: http://localhost:8000/redoc

ğŸ“¡ Endpoint de Chat
POST /chat

Request body (application/json):

{
  "message": "Quanto Ã© 1234 * 5678?"
}


Response body:

{
  "response": "7021792"
}

Exemplo usando curl

Pergunta de cÃ¡lculo (usa a tool calculator):

curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d "{\"message\": \"Quanto Ã© 1234 * 5678?\"}"


Pergunta de conhecimento geral (NÃƒO usa tool):

curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d "{\"message\": \"Quem foi Albert Einstein?\"}"

ğŸ§  LÃ³gica do Agente

O agente Ã© configurado em app/agent.py:

Modelo: OllamaModel, configurado via variÃ¡veis de ambiente.

Tool registrada: calculator (strands-agents-tools).

O comportamento Ã© controlado pelo AGENT_SYSTEM_PROMPT, que instrui:

Quando usar a tool de cÃ¡lculo (expressÃµes matemÃ¡ticas explÃ­citas).

Quando nÃ£o usar a tool (perguntas gerais, biografias, conceitos, etc.).

ğŸ“ ObservaÃ§Ãµes

O projeto foi estruturado para ser simples de executar e fÃ¡cil de ler, destacando:

SeparaÃ§Ã£o entre configuraÃ§Ã£o (config.py), agente (agent.py), API (main.py) e schemas (schemas.py).

Uso de .env e .env.example para configuraÃ§Ã£o do ambiente.

Boas prÃ¡ticas bÃ¡sicas de FastAPI e Pydantic.
